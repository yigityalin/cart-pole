{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7414c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from functools import partial\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "mpl.rc(\"animation\", html=\"jshtml\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f06bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0767269",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_layer = partial(\n",
    "    layers.Dense, kernel_initializer=\"he_normal\", activation=\"elu\"\n",
    ")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    default_layer(32, input_shape=env.observation_space.shape),\n",
    "    default_layer(32),\n",
    "    default_layer(16),\n",
    "    default_layer(8),\n",
    "    default_layer(4),\n",
    "    default_layer(env.action_space.n)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d91c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.98\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.05)\n",
    "mse_loss = keras.losses.MeanSquaredError()\n",
    "\n",
    "replay_buffer = deque(maxlen=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72418cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay_schedule(episode):\n",
    "    if episode < 50:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.reciprocal(np.log2(2 + episode - 50))\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(env.action_space.n)\n",
    "    else:\n",
    "        q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f76119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    samples = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in samples])\n",
    "        for field_index in range(5)\n",
    "    ]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e67f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d556850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, batch_size):\n",
    "    states, actions, rewards, next_states, dones = get_sample_experiences(batch_size)\n",
    "    next_q_values = model.predict(next_states, verbose=0)\n",
    "    next_state_values = next_q_values.max(axis=-1)\n",
    "    q_values = rewards + (1 - dones) * discount_factor * next_state_values\n",
    "    q_values = q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, env.action_space.n)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_q_values = model(states)\n",
    "        predicted_q_values = tf.reduce_sum(\n",
    "            predicted_q_values * mask, axis=1, keepdims=True\n",
    "        )\n",
    "        loss = tf.reduce_mean(mse_loss(q_values, predicted_q_values))\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b50bcf2",
   "metadata": {},
   "outputs": [
    {
     "execution_count": 9,
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2000/2000 [37:46<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "best_score = 0\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    state = env.reset()\n",
    "    for step in range(1, env.spec.max_episode_steps + 1):\n",
    "        epsilon = epsilon_decay_schedule(episode)\n",
    "        state, reward, done, info = play_one_step(state, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "    \n",
    "    if episode > 100:\n",
    "        train_one_step(model, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2928f6d",
   "metadata": {},
   "outputs": [
    {
     "execution_count": 10,
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3f31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, n_episodes=100):\n",
    "    min_steps = env.spec.max_episode_steps\n",
    "    max_steps = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for _ in tqdm(range(n_episodes)):\n",
    "        state = env.reset()\n",
    "        for step in range(1, env.spec.max_episode_steps + 1):\n",
    "            action = epsilon_greedy_policy(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "        total_steps += step\n",
    "        if step < min_steps:\n",
    "            min_steps = step\n",
    "        if step > max_steps:\n",
    "            max_steps = step\n",
    "    \n",
    "    return min_steps, max_steps, total_steps / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8604eda2",
   "metadata": {},
   "outputs": [
    {
     "execution_count": 12,
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [23:08<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(best_weights)\n",
    "min_steps, max_steps, avg_steps = evaluate_policy(env, epsilon_greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c06bcae",
   "metadata": {},
   "outputs": [
    {
     "execution_count": 13,
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of steps in an episode: 500\n",
      "The maximum number of steps in an episode: 500\n",
      "The average number of steps in an episode: 500.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The minimum number of steps in an episode: {min_steps}\")\n",
    "print(f\"The maximum number of steps in an episode: {max_steps}\")\n",
    "print(f\"The average number of steps in an episode: {avg_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ae75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_episode(steps, interval=50, repeat=False):\n",
    "    def update_frame(index):\n",
    "        frame.set_data(steps[index])\n",
    "        return frame,\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    frame = plt.imshow(steps[0])\n",
    "    animation = FuncAnimation(\n",
    "        fig, update_frame, frames=len(steps), interval=interval, repeat=repeat\n",
    "    )\n",
    "    plt.close()\n",
    "    return animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15caf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(env, policy):\n",
    "    steps = []\n",
    "    state = env.reset()\n",
    "    for step in range(env.spec.max_episode_steps):\n",
    "        steps.append(env.render(mode=\"rgb_array\"))\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = play_one_episode(env, epsilon_greedy_policy)\n",
    "plot_one_episode(steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
